#!/usr/bin/env python3
"""
LA-Bench 2025: å®Ÿé¨“æ‰‹é †ç”Ÿæˆã‚¿ã‚¹ã‚¯
Baseline Implementation with Responses API (Standalone Python Script)
GitHub: https://github.com/lasa-or-jp/la-bench.git

Usage:
    export OPENAI_API_KEY="your-api-key"
    python baseline_responses_api.py
"""

import os
import json
import time
from datetime import datetime
from typing import Dict, List, Optional, Set, Any
from pathlib import Path
from dataclasses import dataclass, field
import warnings
warnings.filterwarnings('ignore')

# Data processing
import pandas as pd
from pydantic import BaseModel, Field

# OpenAI API
try:
    from openai import OpenAI
    OPENAI_AVAILABLE = True
except ImportError:
    OPENAI_AVAILABLE = False
    print("âš ï¸ OpenAIãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
    print("pip install openai ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã¦ãã ã•ã„")
    exit(1)

# Progress bar
from tqdm.auto import tqdm

# Logging
import logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger(__name__)


# ============================================================================
# Configuration
# ============================================================================

# Model settings (Responses API)
MODEL_NAME = "gpt-5.1"
REASONING_EFFORT = "medium"  # low, medium, high

# Judge model settings
JUDGE_MODEL = "gpt-4.1-mini"
JUDGE_TEMPERATURE = 0.2

# Input/Output paths
JSONL_PATH = 'data/example/example.jsonl'
OUTPUT_DIR = Path('./outputs/runs')


# ============================================================================
# Data Structures
# ============================================================================

@dataclass
class Step:
    id: int
    text: str


@dataclass
class ReferenceEntry:
    id: int
    text: str


@dataclass
class ExampleInput:
    instruction: str
    mandatory_objects: Set[str] = field(default_factory=set)
    source_protocol_steps: List[Step] = field(default_factory=list)
    expected_final_states: Set[str] = field(default_factory=set)
    references: List[ReferenceEntry] = field(default_factory=list)


@dataclass
class ExampleOutput:
    procedure_steps: List[Step] = field(default_factory=list)


@dataclass
class Measurement:
    specific_criteria: Dict[str, int] = field(default_factory=dict)


@dataclass
class ExampleSample:
    id: str
    input: ExampleInput
    output: ExampleOutput
    measurement: Optional[Measurement] = None


# ============================================================================
# Pydantic Models for Structured Output
# ============================================================================

class StepModel(BaseModel):
    id: int = Field(ge=1, description="ã‚¹ãƒ†ãƒƒãƒ—ç•ªå·")
    text: str = Field(description="å®Ÿé¨“æ‰‹é †ã®è©³ç´°ãªèª¬æ˜")


class GeneratedOutput(BaseModel):
    procedure_steps: List[StepModel] = Field(
        description="å®Ÿé¨“æ‰‹é †ã®ãƒªã‚¹ãƒˆ",
        min_items=1,
        max_items=50
    )


class JudgeOutput(BaseModel):
    general_score: float = Field(ge=0, le=5)
    specific_score: float = Field(ge=0, le=5)
    final_score: float = Field(ge=0, le=10)
    general_reason: str
    specific_matches: List[str] = []
    notes: Optional[str] = None


# ============================================================================
# Helper Functions
# ============================================================================

def _to_set(x):
    return set(x) if isinstance(x, (list, set, tuple)) else set()


def _to_list(x):
    return list(x) if isinstance(x, (list, set, tuple)) else (x if isinstance(x, list) else [])


def _to_steps(x) -> List[Step]:
    steps: List[Step] = []
    arr = _to_list(x)
    if not arr:
        return steps
    if isinstance(arr[0], dict):
        for it in arr:
            try:
                sid = int(it.get("id", len(steps) + 1))
            except Exception:
                sid = len(steps) + 1
            steps.append(Step(id=sid, text=str(it.get("text", "")).strip()))
    else:
        for idx, s in enumerate(arr, start=1):
            steps.append(Step(id=idx, text=str(s).strip()))
    return steps


def _to_references(x) -> List[ReferenceEntry]:
    refs: List[ReferenceEntry] = []
    arr = _to_list(x)
    if not arr:
        return refs
    if isinstance(arr[0], dict):
        for it in arr:
            try:
                rid = int(it.get("id", len(refs) + 1))
            except Exception:
                rid = len(refs) + 1
            refs.append(ReferenceEntry(id=rid, text=str(it.get("text", "")).strip()))
    else:
        for idx, ref in enumerate(arr, start=1):
            refs.append(ReferenceEntry(id=idx, text=str(ref).strip()))
    return refs


def parse_sample(obj: Dict[str, Any]) -> ExampleSample:
    sid = obj.get("id") or obj.get("sample_id") or "unknown"
    i = obj.get("input", {})
    o = obj.get("output", {})
    m = obj.get("measurement", {})

    # Measurement.specific_criteria ã‚’ dict ã«æ­£è¦åŒ–ï¼ˆlistå½¢å¼ã‚‚è¨±å®¹ï¼‰
    sc_raw = m.get("specific_criteria", {})
    sc: Dict[str, int] = {}
    if isinstance(sc_raw, dict):
        for k, v in sc_raw.items():
            try:
                sc[str(k)] = int(v)
            except Exception:
                pass
    elif isinstance(sc_raw, list):
        for it in sc_raw:
            try:
                k = it.get("item")
                v = int(it.get("score", 0))
                if k:
                    sc[str(k)] = v
            except Exception:
                pass

    sample = ExampleSample(
        id=str(sid),
        input=ExampleInput(
            instruction=str(i.get("instruction", "")).strip(),
            mandatory_objects=_to_set(i.get("mandatory_objects", [])),
            source_protocol_steps=_to_steps(i.get("source_protocol_steps", [])),
            expected_final_states=_to_set(i.get("expected_final_states", [])),
            references=_to_references(i.get("references", [])),
        ),
        output=ExampleOutput(
            procedure_steps=_to_steps(o.get("procedure_steps", []))
        ),
        measurement=Measurement(specific_criteria=sc) if sc else None
    )
    return sample


def load_example_jsonl(path: str):
    samples = []
    p = Path(path)
    if not p.exists():
        raise FileNotFoundError(f"JSONL not found: {p}")
    for line in p.read_text(encoding="utf-8").splitlines():
        line = line.strip()
        if not line:
            continue
        try:
            obj = json.loads(line)
        except Exception as e:
            print(f"âš ï¸ JSONL parse error: {e}")
            continue
        samples.append(parse_sample(obj))
    return samples


# ============================================================================
# Generation Functions (Responses API)
# ============================================================================

def build_input_text(sample: ExampleSample) -> str:
    """
    Responses APIç”¨ã«å˜ä¸€ã®å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆã‚’æ§‹ç¯‰
    """
    lines = []
    lines.append("ã‚ãªãŸã¯ç”Ÿå‘½ç§‘å­¦å®Ÿé¨“ã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã® Input ã‚’èª­ã¿ã€")
    lines.append("æ—¥æœ¬èªã§å®Ÿè¡Œå¯èƒ½ãªå®Ÿé¨“æ‰‹é †ï¼ˆprocedure_stepsï¼‰ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚")
    lines.append("åˆ¶ç´„: ã‚¹ãƒ†ãƒƒãƒ—æ•°ã¯æœ€å¤§50ã€å„ã‚¹ãƒ†ãƒƒãƒ—ã¯10æ–‡ä»¥ä¸‹ã€idã¯1ã‹ã‚‰æ˜‡é †ã€‚")
    lines.append("")
    lines.append(f"ã€å®Ÿé¨“æŒ‡ç¤ºã€‘\n{sample.input.instruction}")

    if sample.input.mandatory_objects:
        lines.append("\nã€ä½¿ç”¨ã™ã‚‹ç‰©å“ã€‘")
        for it in sorted(sample.input.mandatory_objects):
            lines.append(f"- {it}")

    if sample.input.source_protocol_steps:
        lines.append("\nã€å…ƒãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®æ‰‹é †ï¼ˆå‚è€ƒï¼‰ã€‘")
        for st in sample.input.source_protocol_steps:
            lines.append(f"- {st.id}. {st.text}")

    if sample.input.expected_final_states:
        lines.append("\nã€æœŸå¾…ã•ã‚Œã‚‹æœ€çµ‚çŠ¶æ…‹ã€‘")
        for fs in sorted(sample.input.expected_final_states):
            lines.append(f"- {fs}")

    if sample.input.references:
        lines.append("\nã€å‚è€ƒæ–‡çŒ®ã€‘")
        for ref in sample.input.references:
            lines.append(f"- [{ref.id}] {ref.text}")

    return "\n".join(lines)


def generate_outputs(samples: list[ExampleSample], api_key: str) -> list[dict]:
    client = OpenAI(api_key=api_key)
    results: list[dict] = []

    for sm in tqdm(samples, desc="Generating procedures (Responses API)"):
        input_text = build_input_text(sm)
        try:
            # Responses APIã‚’ä½¿ç”¨ã—ã¦reasoning effortã‚’åˆ¶å¾¡
            response = client.responses.create(
                model=MODEL_NAME,
                input=input_text,
                reasoning={"effort": REASONING_EFFORT},
                response_format=GeneratedOutput,
            )

            # æ§‹é€ åŒ–ã•ã‚ŒãŸå‡ºåŠ›ã‚’ãƒ‘ãƒ¼ã‚¹
            parsed: GeneratedOutput = response.parsed_output
            steps = [
                Step(id=s.id, text=s.text)
                for s in sorted(parsed.procedure_steps, key=lambda x: x.id)
            ][:50]

        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±æ•—: {sm.id}: {e}")
            steps = []

        results.append({
            "id": sm.id,
            "procedure_steps": [{"id": s.id, "text": s.text} for s in steps],
        })

    print(f"âœ… ç”Ÿæˆå®Œäº†: {len(results)} samples (reasoning={REASONING_EFFORT})")
    return results


# ============================================================================
# Evaluation Functions
# ============================================================================

def build_judge_messages(sample: ExampleSample, steps: List[Step]) -> list[dict]:
    system = (
        "ã‚ãªãŸã¯ç”Ÿå‘½ç§‘å­¦å®Ÿé¨“ã®å°‚é–€å®¶ã§ã‚ã‚Šã€å…¬å¹³ãªæ¡ç‚¹è€…ã§ã™ã€‚"
        "ä»¥ä¸‹ã®åŸºæº–ã«å¾“ã£ã¦ã€ä¸ãˆã‚‰ã‚ŒãŸ Input ã¨ç”Ÿæˆæ‰‹é †ï¼ˆOutputï¼‰ã‚’è©•ä¾¡ã—ã€"
        "general_score(0-5) ã¨ specific_score(0-5) ã¨ final_score(0-10) ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
        "\n\n[å…±é€šæ¡ç‚¹åŸºæº– 5ç‚¹æº€ç‚¹]\n"
        "åŠ ç‚¹(+1ãšã¤): 1) å®Ÿé¨“æŒ‡ç¤ºã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åæ˜ , 2) ä½¿ç”¨ã™ã‚‹ç‰©å“ã®åæ˜ , 3) å…ƒæ‰‹é †ã®è«–ç†åæ˜ , 4) æœŸå¾…ã•ã‚Œã‚‹æœ€çµ‚çŠ¶æ…‹ã®é”æˆ, 5) é©åˆ‡ãªè£œå®Œã€‚\n"
        "æ¸›ç‚¹: ä¸è‡ªç„¶ãªæ—¥æœ¬èª/ãƒãƒ«ã‚·ãƒãƒ¼ã‚·ãƒ§ãƒ³, è¨ˆç®—ãƒŸã‚¹, æ‰‹é †çŸ›ç›¾ã€‚\n"
        "ä¸Šé™: å…¥åŠ›æ‰‹é †ã®ä¸¸å†™ã—ç­‰ã®éåº¦ã®å®‰å…¨æ€§ãŒè¦‹ã‚‰ã‚Œã‚‹å ´åˆã€general_score ã¯æœ€å¤§2ç‚¹ã«åˆ¶é™ã€‚\n\n"
        "[å€‹åˆ¥æ¡ç‚¹åŸºæº– 5ç‚¹æº€ç‚¹]\n"
        "ä¸ãˆã‚‰ã‚ŒãŸ specific_criteria ã®å„ item ãŒæ‰‹é †ã«å«ã¾ã‚Œã‚‹/æº€ãŸã™ãªã‚‰ã€ãã® score ã‚’åŠ ç‚¹ï¼ˆåˆè¨ˆ5ç‚¹ã§ä¸Šé™ï¼‰ã€‚"
    )

    parts = []
    parts.append(f"ã€å®Ÿé¨“æŒ‡ç¤ºã€‘\n{sample.input.instruction}")
    if sample.input.mandatory_objects:
        parts.append("\nã€ä½¿ç”¨ã™ã‚‹ç‰©å“ã€‘")
        for it in sorted(sample.input.mandatory_objects):
            parts.append(f"- {it}")
    if sample.input.source_protocol_steps:
        parts.append("\nã€å…ƒãƒ—ãƒ­ãƒˆã‚³ãƒ«ã®æ‰‹é †ï¼ˆå‚è€ƒï¼‰ã€‘")
        for st in sample.input.source_protocol_steps:
            parts.append(f"- {st.id}. {st.text}")
    if sample.input.expected_final_states:
        parts.append("\nã€æœŸå¾…ã•ã‚Œã‚‹æœ€çµ‚çŠ¶æ…‹ã€‘")
        for fs in sorted(sample.input.expected_final_states):
            parts.append(f"- {fs}")
    if sample.input.references:
        parts.append("\nã€å‚è€ƒæ–‡çŒ®ã€‘")
        for ref in sample.input.references:
            parts.append(f"- [{ref.id}] {ref.text}")

    parts.append("\nã€ç”Ÿæˆæ‰‹é †ï¼ˆOutputï¼‰ã€‘")
    for s in steps:
        parts.append(f"- {s.id}. {s.text}")

    parts.append("\nã€specific_criteriaã€‘")
    if sample.measurement and sample.measurement.specific_criteria:
        for item, sc in sample.measurement.specific_criteria.items():
            parts.append(f"- ({int(sc)}ç‚¹) {item}")
    else:
        parts.append("- ãªã—")

    user = "\n".join(parts)
    return [
        {"role": "system", "content": system},
        {"role": "user", "content": user},
    ]


def judge_with_llm(samples: List[ExampleSample], generated: list[dict], api_key: str) -> pd.DataFrame:
    client = OpenAI(api_key=api_key)
    proc_map = {g['id']: [Step(id=it['id'], text=it['text']) for it in g['procedure_steps']] for g in generated}
    rows = []
    quota_exhausted = False

    def _is_insufficient_quota(err: Exception) -> bool:
        s = str(err)
        return 'insufficient_quota' in s or 'You exceeded your current quota' in s

    for sm in tqdm(samples, desc="Evaluating procedures"):
        if quota_exhausted:
            print(f"â­ï¸ ã‚¹ã‚­ãƒƒãƒ—æ¡ç‚¹: {sm.id}ï¼ˆã‚¯ã‚©ãƒ¼ã‚¿ä¸è¶³ï¼‰")
            rows.append({
                'id': sm.id,
                'general_score': 0.0,
                'specific_score': 0.0,
                'total_score': 0.0,
                'notes': 'skipped_due_to_quota',
            })
            continue
        steps = proc_map.get(sm.id, [])
        msgs = build_judge_messages(sm, steps)
        try:
            completion = client.chat.completions.parse(
                model=JUDGE_MODEL,
                messages=msgs,
                temperature=JUDGE_TEMPERATURE,
                response_format=JudgeOutput,
            )
            parsed: JudgeOutput = completion.choices[0].message.parsed
            rows.append({
                'id': sm.id,
                'general_score': parsed.general_score,
                'specific_score': parsed.specific_score,
                'total_score': parsed.final_score,
                'notes': parsed.notes or '',
            })
        except Exception as e:
            print(f"âŒ è©•ä¾¡å¤±æ•—: {sm.id}: {e}")
            if _is_insufficient_quota(e):
                print("âš ï¸ APIã‚¯ã‚©ãƒ¼ã‚¿ä¸è¶³ã®ãŸã‚ã€ä»¥é™ã®æ¡ç‚¹ã‚’ä¸­æ–­ã—ã¾ã™ã€‚ãƒ—ãƒ©ãƒ³/èª²é‡‘è¨­å®šã‚’ã”ç¢ºèªãã ã•ã„ã€‚")
                quota_exhausted = True
            rows.append({
                'id': sm.id,
                'general_score': 0.0,
                'specific_score': 0.0,
                'total_score': 0.0,
                'notes': 'evaluation_failed',
            })
    return pd.DataFrame(rows)


# ============================================================================
# Main Function
# ============================================================================

def main():
    print("=" * 60)
    print("LA-Bench 2025 Baseline Implementation (Responses API)")
    print(f"å®Ÿè¡Œæ™‚åˆ»: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("=" * 60)

    # Get API key from environment
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        print("âŒ Error: OPENAI_API_KEYç’°å¢ƒå¤‰æ•°ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
        print("è¨­å®šæ–¹æ³•: export OPENAI_API_KEY='your-api-key'")
        exit(1)

    print(f"âœ… OpenAI API Key: {'*' * 20}{api_key[-4:]}")
    print(f"ğŸ“Š Model: {MODEL_NAME}")
    print(f"ğŸ§  Reasoning Effort: {REASONING_EFFORT}")
    print()

    # Load samples
    try:
        samples = load_example_jsonl(JSONL_PATH)
        print(f'âœ… Loaded {len(samples)} samples from {JSONL_PATH}')
    except Exception as e:
        print(f'âŒ Load error: {e}')
        exit(1)

    # Generate outputs
    print("\n" + "=" * 60)
    print("Step 1: å®Ÿé¨“æ‰‹é †ã®ç”Ÿæˆ (Responses API)")
    print("=" * 60)
    generated_results = generate_outputs(samples, api_key)
    if generated_results:
        print(f"ä¾‹: {generated_results[0]['id']} â†’ {len(generated_results[0]['procedure_steps'])} steps")

    # Save generated results to JSONL
    ts = time.strftime('%Y%m%d_%H%M%S')
    OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    jsonl_path = OUTPUT_DIR / f'generated_responses_{ts}.jsonl'
    with jsonl_path.open('w', encoding='utf-8') as f:
        for rec in generated_results:
            obj = {"id": rec["id"], "output": {"procedure_steps": rec["procedure_steps"]}}
            line = json.dumps(obj, ensure_ascii=False, separators=(",", ":"))
            f.write(line + "\n")
    print(f"ğŸ“„ Saved JSONL: {jsonl_path}")

    # Evaluate with LLM-as-a-judge
    print("\n" + "=" * 60)
    print("Step 2: LLM-as-a-judge è©•ä¾¡")
    print("=" * 60)
    df = judge_with_llm(samples, generated_results, api_key)
    print(f"âœ… LLM-as-a-judge: Scored {len(df)} samples (0-10)")
    print("\nè©•ä¾¡çµæœ:")
    print(df[['id', 'general_score', 'specific_score', 'total_score']])

    # Save evaluation results to CSV
    csv_path = OUTPUT_DIR / f'eval_responses_{ts}.csv'
    df.to_csv(csv_path, index=False, encoding="utf_8_sig")
    print(f'\nğŸ“„ Saved CSV: {csv_path}')

    print("\n" + "=" * 60)
    print("âœ… å‡¦ç†å®Œäº†")
    print("=" * 60)


if __name__ == "__main__":
    main()
